<!DOCTYPE html>
<html lang="zh" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>机器学习习题集 | Minjie&#39;s Blog</title>
<meta name="keywords" content="机器学习, 数学推导, 算法">
<meta name="description" content="本习题集包含16道经典机器学习问题的完整数学推导，
从梯度下降、交叉熵损失、最大似然估计，
到核方法、支持向量机对偶问题、信息增益比计算，
再到 XGBoost 的二阶泰勒展开。
每道题目都配有详细的解题步骤和数学证明，
帮助读者深入理解机器学习算法的理论基础。
">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/dase-course/ml-exercise-set/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.css" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/code.png">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/code.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/code.png">
<link rel="apple-touch-icon" href="http://localhost:1313/code.png">
<link rel="mask-icon" href="http://localhost:1313/code.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="zh" href="http://localhost:1313/dase-course/ml-exercise-set/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:ital,wght@0,100..800;1,100..800&display=swap" rel="stylesheet">
<script type="text/javascript"
        async
        src="https://cdn.bootcss.com/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$'], ['\[\[','\]\]']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
});

MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<style>
code.has-jax {
    font: inherit;
    font-size: 100%;
    background: inherit;
    border: inherit;
    color: #515151;
}
</style>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Minjie&#39;s Blog (Alt + H)">Minjie&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/" title="Minjie&#39;s Blog">
                    <span>首页</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/archives/" title="归档">
                    <span>归档</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>分类</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>标签</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="搜索">
                    <span>搜索</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="关于我">
                    <span>关于</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      机器学习习题集
    </h1>
    <div class="post-description">
      本习题集包含16道经典机器学习问题的完整数学推导，
从梯度下降、交叉熵损失、最大似然估计，
到核方法、支持向量机对偶问题、信息增益比计算，
再到 XGBoost 的二阶泰勒展开。
每道题目都配有详细的解题步骤和数学证明，
帮助读者深入理解机器学习算法的理论基础。

    </div>
    <div class="post-meta"><span title='2025-12-13 20:26:19 +0800 CST'>2025年12月13日</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">目录</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%80%e9%a2%98%e7%ba%bf%e6%80%a7%e5%9b%9e%e5%bd%92%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8d" aria-label="第一题：线性回归梯度下降">第一题：线性回归梯度下降</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%ba%8c%e9%a2%98%e4%ba%a4%e5%8f%89%e7%86%b5%e6%8d%9f%e5%a4%b1%e6%a2%af%e5%ba%a6" aria-label="第二题：交叉熵损失梯度">第二题：交叉熵损失梯度</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%89%e9%a2%98%e9%ab%98%e6%96%af%e5%81%87%e8%ae%be%e4%b8%8b%e7%9a%84%e6%9c%80%e5%a4%a7%e4%bc%bc%e7%84%b6%e4%bc%b0%e8%ae%a1" aria-label="第三题：高斯假设下的最大似然估计">第三题：高斯假设下的最大似然估计</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%9b%9b%e9%a2%98logistic%e5%9b%9e%e5%bd%92%e7%9a%84nll%e6%8d%9f%e5%a4%b1" aria-label="第四题：Logistic回归的NLL损失">第四题：Logistic回归的NLL损失</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%ba%94%e9%a2%98poisson%e5%88%86%e5%b8%83%e7%9a%84%e6%8c%87%e6%95%b0%e6%97%8f%e5%bd%a2%e5%bc%8f" aria-label="第五题：Poisson分布的指数族形式">第五题：Poisson分布的指数族形式</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%85%ad%e9%a2%98shapley%e5%80%bc%e8%ae%a1%e7%ae%97" aria-label="第六题：Shapley值计算">第六题：Shapley值计算</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%b8%83%e9%a2%98%e5%8d%8f%e6%96%b9%e5%b7%ae%e7%9f%a9%e9%98%b5%e6%80%a7%e8%b4%a8" aria-label="第七题：协方差矩阵性质">第七题：协方差矩阵性质</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%85%ab%e9%a2%98%e9%ab%98%e6%96%af%e5%88%a4%e5%88%ab%e5%88%86%e6%9e%90%e7%9a%84mle" aria-label="第八题：高斯判别分析的MLE">第八题：高斯判别分析的MLE</a></li>
                <li>
                    <a href="#%e7%ac%ac%e4%b9%9d%e9%a2%98gda%e5%8f%af%e8%bd%ac%e5%8c%96%e4%b8%balogistic%e5%9b%9e%e5%bd%92" aria-label="第九题：GDA可转化为Logistic回归">第九题：GDA可转化为Logistic回归</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e9%a2%98kernel-method%e5%88%86%e6%9e%90" aria-label="第十题：Kernel Method分析">第十题：Kernel Method分析</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e4%b8%80%e9%a2%98%e8%b6%85%e5%b9%b3%e9%9d%a2%e7%9a%84%e5%87%bd%e6%95%b0%e9%97%b4%e9%9a%94%e5%92%8c%e5%87%a0%e4%bd%95%e9%97%b4%e9%9a%94" aria-label="第十一题：超平面的函数间隔和几何间隔">第十一题：超平面的函数间隔和几何间隔</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e4%ba%8c%e9%a2%98svm%e7%9a%84lagrange%e5%87%bd%e6%95%b0%e5%92%8c%e5%af%b9%e5%81%b6%e5%bd%a2%e5%bc%8f" aria-label="第十二题：SVM的Lagrange函数和对偶形式">第十二题：SVM的Lagrange函数和对偶形式</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e4%b8%89%e9%a2%98%e7%ba%bf%e6%80%a7%e4%b8%8d%e5%8f%af%e5%88%86%e7%9a%84svm%e4%b8%8el1%e6%ad%a3%e5%88%99" aria-label="第十三题：线性不可分的SVM与L1正则">第十三题：线性不可分的SVM与L1正则</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e5%9b%9b%e9%a2%98svm%e6%9c%80%e4%bc%98%e5%8c%96%e9%97%ae%e9%a2%98%e5%88%86%e6%9e%90" aria-label="第十四题：SVM最优化问题分析">第十四题：SVM最优化问题分析</a></li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e4%ba%94%e9%a2%98%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a%e6%af%94%e8%ae%a1%e7%ae%97" aria-label="第十五题：信息增益比计算">第十五题：信息增益比计算</a><ul>
                        
                <li>
                    <a href="#%e7%89%b9%e5%be%811%e5%b9%b4%e9%be%84" aria-label="特征1：年龄">特征1：年龄</a></li>
                <li>
                    <a href="#%e7%89%b9%e5%be%812%e6%9c%89%e5%b7%a5%e4%bd%9c" aria-label="特征2：有工作">特征2：有工作</a></li>
                <li>
                    <a href="#%e7%89%b9%e5%be%813%e6%9c%89%e8%87%aa%e5%b7%b1%e7%9a%84%e6%88%bf%e5%ad%90" aria-label="特征3：有自己的房子">特征3：有自己的房子</a></li>
                <li>
                    <a href="#%e7%89%b9%e5%be%814%e4%bf%a1%e8%b4%b7%e6%83%85%e5%86%b5" aria-label="特征4：信贷情况">特征4：信贷情况</a></li>
                <li>
                    <a href="#%e6%80%bb%e7%bb%93%e4%bf%a1%e6%81%af%e5%a2%9e%e7%9b%8a%e6%af%94%e6%8e%92%e5%ba%8f" aria-label="总结（信息增益比排序）">总结（信息增益比排序）</a></li></ul>
                </li>
                <li>
                    <a href="#%e7%ac%ac%e5%8d%81%e5%85%ad%e9%a2%98xgboost%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0%e4%ba%8c%e9%98%b6%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80" aria-label="第十六题：XGBoost损失函数二阶泰勒展开">第十六题：XGBoost损失函数二阶泰勒展开</a><ul>
                        
                <li>
                    <a href="#%e4%ba%8c%e9%98%b6%e6%b3%b0%e5%8b%92%e5%b1%95%e5%bc%80" aria-label="二阶泰勒展开">二阶泰勒展开</a></li>
                <li>
                    <a href="#%e6%8e%a8%e5%af%bc%e5%8f%b6%e5%ad%90%e6%9d%83%e9%87%8d" aria-label="推导叶子权重 $w_j^*$">推导叶子权重 $w_j^*$</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="第一题线性回归梯度下降">第一题：线性回归梯度下降<a hidden class="anchor" aria-hidden="true" href="#第一题线性回归梯度下降">#</a></h2>
<p><strong>题目：</strong> 对线性模型 $h_\theta(x) = \theta^\top x$，给定训练集 $\{(x^{(i)}, y^{(i)})\}$，推导其向量形式的最小二乘损失梯度下降更新公式为：</p>
$$\theta := \theta + \alpha \sum_{i=1}^{n} (y^{(i)} - h_\theta(x^{(i)})) x^{(i)}$$<p><strong>解：</strong></p>
<p>最小二乘损失函数为：</p>
$$J(\theta) = \frac{1}{2}\sum_{i=1}^{n}(h_\theta(x^{(i)}) - y^{(i)})^2 = \frac{1}{2}\sum_{i=1}^{n}(\theta^\top x^{(i)} - y^{(i)})^2$$<p>对 $\theta$ 求梯度：</p>
$$\begin{aligned} \nabla_\theta J(\theta) &= \sum_{i=1}^{n}(\theta^\top x^{(i)} - y^{(i)}) \cdot x^{(i)} \\ &= \sum_{i=1}^{n}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \end{aligned}$$<p>梯度下降更新规则为 $\theta := \theta - \alpha \nabla_\theta J(\theta)$，因此：</p>
$$\theta := \theta - \alpha \sum_{i=1}^{n}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} = \theta + \alpha \sum_{i=1}^{n}(y^{(i)} - h_\theta(x^{(i)})) x^{(i)}$$<hr>
<h2 id="第二题交叉熵损失梯度">第二题：交叉熵损失梯度<a hidden class="anchor" aria-hidden="true" href="#第二题交叉熵损失梯度">#</a></h2>
<p><strong>题目：</strong> Cross Entropy Loss 定义如下：</p>
$$l_{ce}((t_1,\ldots,t_k),y) = -\log\left(\frac{\exp(t_y)}{\sum_j \exp(t_j)}\right)$$<p>令向量 $t = (t_1,t_2,\ldots,t_k)$，推导 CEL 对任意 $t_i$ 求导为：</p>
$$\frac{\partial l_{ce}(t,y)}{\partial t_i} = \phi_i - \mathbb{1}\{y=i\}$$<p><strong>解：</strong></p>
<p>记 $\phi_i = \frac{\exp(t_i)}{\sum_j \exp(t_j)}$ 为 softmax 函数。</p>
<p>首先简化损失函数：</p>
$$l_{ce}(t,y) = -\log(\phi_y) = -t_y + \log\left(\sum_j \exp(t_j)\right)$$<p>对 $t_i$ 求导：</p>
$$\begin{aligned} \frac{\partial l_{ce}(t,y)}{\partial t_i} &= -\frac{\partial t_y}{\partial t_i} + \frac{\partial}{\partial t_i}\log\left(\sum_j \exp(t_j)\right) \\ &= -\mathbb{1}\{y=i\} + \frac{\exp(t_i)}{\sum_j \exp(t_j)} \\ &= \phi_i - \mathbb{1}\{y=i\} \end{aligned}$$<p>其中 $\mathbb{1}\{y=i\}$ 是指示函数，当 $y=i$ 时为1，否则为0。</p>
<hr>
<h2 id="第三题高斯假设下的最大似然估计">第三题：高斯假设下的最大似然估计<a hidden class="anchor" aria-hidden="true" href="#第三题高斯假设下的最大似然估计">#</a></h2>
<p><strong>题目：</strong> 证明在高斯差异假定下，对线性模型 $h_\theta(x) = \theta^\top x$，最大化参数似然 $L(\theta)$ 等价于最小化二乘损失 $\sum_{i=1}^{n}(y^{(i)} - \theta^\top x^{(i)})^2$。</p>
<p><strong>解：</strong></p>
<p>假设误差 $\epsilon^{(i)} = y^{(i)} - \theta^\top x^{(i)}$ 服从独立同分布的高斯分布 $\mathcal{N}(0, \sigma^2)$，即：</p>
$$p(\epsilon^{(i)}) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right)$$<p>因此：</p>
$$p(y^{(i)} | x^{(i)}; \theta) = \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)} - \theta^\top x^{(i)})^2}{2\sigma^2}\right)$$<p>似然函数为：</p>
$$\begin{aligned} L(\theta) &= \prod_{i=1}^{n} p(y^{(i)} | x^{(i)}; \theta) \\ &= \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)} - \theta^\top x^{(i)})^2}{2\sigma^2}\right) \end{aligned}$$<p>对数似然为：</p>
$$\begin{aligned} \log L(\theta) &= \sum_{i=1}^{n}\left[\log\frac{1}{\sqrt{2\pi}\sigma} - \frac{(y^{(i)} - \theta^\top x^{(i)})^2}{2\sigma^2}\right] \\ &= n\log\frac{1}{\sqrt{2\pi}\sigma} - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(y^{(i)} - \theta^\top x^{(i)})^2 \end{aligned}$$<p>最大化 $\log L(\theta)$ 等价于最小化 $\sum_{i=1}^{n}(y^{(i)} - \theta^\top x^{(i)})^2$。</p>
<hr>
<h2 id="第四题logistic回归的nll损失">第四题：Logistic回归的NLL损失<a hidden class="anchor" aria-hidden="true" href="#第四题logistic回归的nll损失">#</a></h2>
<p><strong>题目：</strong> 对Logistic回归模型 $h_\theta(x) = g(\theta^\top x) = \frac{1}{1+e^{-\theta^\top x}}$，推导其在单样本 $(x,y)$ 下的NLL（negative log likelihood）损失，以及损失对特定参数 $\theta_j$ 的导数为 $(h_\theta(x) - y)x_j$。</p>
<p>提示：Logistic回归预测概率的统一形式为 $P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$</p>
<p><strong>解：</strong></p>
<p>根据提示，Logistic回归的概率模型为：</p>
$$P(y|x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}$$<p>其中 $y \in \{0,1\}$，$h_\theta(x) = g(\theta^\top x) = \frac{1}{1+e^{-\theta^\top x}}$。</p>
<p>对数似然为：</p>
$$\log P(y|x;\theta) = y\log(h_\theta(x)) + (1-y)\log(1-h_\theta(x))$$<p>NLL损失为：</p>
$$\text{NLL}(x,y;\theta) = -\log P(y|x;\theta) = -y\log(h_\theta(x)) - (1-y)\log(1-h_\theta(x))$$<p>对 $\theta_j$ 求导。首先注意到：</p>
$$\frac{\partial h_\theta(x)}{\partial \theta_j} = h_\theta(x)(1-h_\theta(x)) \cdot x_j$$<p>这是因为 $g'(z) = g(z)(1-g(z))$。</p>
<p>因此：</p>
$$\begin{aligned} \frac{\partial \text{NLL}}{\partial \theta_j} &= -y\frac{1}{h_\theta(x)}\frac{\partial h_\theta(x)}{\partial \theta_j} - (1-y)\frac{1}{1-h_\theta(x)}\left(-\frac{\partial h_\theta(x)}{\partial \theta_j}\right) \\ &= -y\frac{1}{h_\theta(x)} \cdot h_\theta(x)(1-h_\theta(x))x_j + (1-y)\frac{1}{1-h_\theta(x)} \cdot h_\theta(x)(1-h_\theta(x))x_j \\ &= -y(1-h_\theta(x))x_j + (1-y)h_\theta(x)x_j \\ &= (h_\theta(x) - y)x_j \end{aligned}$$<hr>
<h2 id="第五题poisson分布的指数族形式">第五题：Poisson分布的指数族形式<a hidden class="anchor" aria-hidden="true" href="#第五题poisson分布的指数族形式">#</a></h2>
<p><strong>题目：</strong> 已知指数分布族定义如下：$p(y;\eta) = b(y)\exp(\eta^\top y - a(\eta))$。推导Poisson分布的指数分布族形式，并构建Poisson分布对应的广义线性模型。其中，Poisson分布 $\text{Pois}(\lambda)$ 的概率密度函数如下：</p>
$$P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$$<p><strong>解：</strong></p>
<p>将Poisson分布改写为指数族形式：</p>
$$\begin{aligned} P(X=k) &= \frac{\lambda^k e^{-\lambda}}{k!} \\ &= \frac{1}{k!}\exp(k\log\lambda - \lambda) \\ &= \frac{1}{k!}\exp(\eta \cdot k - e^\eta) \end{aligned}$$<p>其中 $\eta = \log\lambda$（自然参数），因此 $\lambda = e^\eta$。</p>
<p>对应指数族形式：</p>
<ul>
<li>$b(y) = \frac{1}{y!}$</li>
<li>$\eta = \log\lambda$</li>
<li>$a(\eta) = e^\eta = \lambda$</li>
<li>$y$ 的充分统计量就是 $y$ 本身</li>
</ul>
<p>构建广义线性模型：</p>
<ol>
<li>假设 $y|x;\theta \sim \text{Pois}(\lambda)$</li>
<li>自然参数 $\eta = \theta^\top x$</li>
<li>因为 $\lambda = e^\eta$，所以 $\lambda = e^{\theta^\top x}$</li>
<li>响应函数（期望）为：$h_\theta(x) = \mathbb{E}[y|x;\theta] = \lambda = e^{\theta^\top x}$</li>
</ol>
<p>这就是Poisson回归模型。</p>
<hr>
<h2 id="第六题shapley值计算">第六题：Shapley值计算<a hidden class="anchor" aria-hidden="true" href="#第六题shapley值计算">#</a></h2>
<p><strong>题目：</strong> 计算以下3人团队的Shapley值 $\phi_1$、$\phi_2$、$\phi_3$。</p>
<p>给定：</p>
<ul>
<li>$C_{123} = 10000$，$C_0 = 0$</li>
<li>$C_{12} = 7500$，$C_{13} = 7500$，$C_{23} = 5000$</li>
<li>$C_1 = 5000$，$C_2 = 5000$，$C_3 = 0$</li>
</ul>
<p><strong>解：</strong></p>
<p>Shapley值的公式为：</p>
$$\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!}[C(S \cup \{i\}) - C(S)]$$<p>对于3人团队，$|N| = 3$，计算每个玩家的边际贡献：</p>
<p><strong>玩家1的Shapley值：</strong></p>
$$\begin{aligned} \phi_1 &= \frac{0!2!}{3!}[C_1 - C_0] + \frac{1!1!}{3!}[C_{12} - C_2] + \frac{1!1!}{3!}[C_{13} - C_3] + \frac{2!0!}{3!}[C_{123} - C_{23}] \\ &= \frac{1}{3}[5000 - 0] + \frac{1}{6}[7500 - 5000] + \frac{1}{6}[7500 - 0] + \frac{1}{3}[10000 - 5000] \\ &= \frac{5000}{3} + \frac{2500}{6} + \frac{7500}{6} + \frac{5000}{3} \\ &= \frac{10000}{3} + \frac{10000}{6} = \frac{20000 + 10000}{6} = 5000 \end{aligned}$$<p><strong>玩家2的Shapley值：</strong></p>
$$\begin{aligned} \phi_2 &= \frac{0!2!}{3!}[C_2 - C_0] + \frac{1!1!}{3!}[C_{12} - C_1] + \frac{1!1!}{3!}[C_{23} - C_3] + \frac{2!0!}{3!}[C_{123} - C_{13}] \\ &= \frac{1}{3}[5000 - 0] + \frac{1}{6}[7500 - 5000] + \frac{1}{6}[5000 - 0] + \frac{1}{3}[10000 - 7500] \\ &= \frac{5000}{3} + \frac{2500}{6} + \frac{5000}{6} + \frac{2500}{3} \\ &= \frac{15000 + 2500 + 5000 + 5000}{6} = \frac{27500}{6} \approx 4583.33 \end{aligned}$$<p><strong>玩家3的Shapley值：</strong></p>
<p>由对称性或直接计算：</p>
$$\phi_3 = 10000 - \phi_1 - \phi_2 = 10000 - 5000 - 4583.33 = 416.67$$<p>或直接计算：</p>
$$\begin{aligned} \phi_3 &= \frac{1}{3}[0] + \frac{1}{6}[7500 - 5000] + \frac{1}{6}[5000 - 5000] + \frac{1}{3}[10000 - 7500] \\ &= 0 + \frac{2500}{6} + 0 + \frac{2500}{3} = \frac{5000}{6} \approx 416.67 \end{aligned}$$<p><strong>答案：</strong> $\phi_1 = 5000$，$\phi_2 \approx 4583.33$，$\phi_3 \approx 416.67$</p>
<hr>
<h2 id="第七题协方差矩阵性质">第七题：协方差矩阵性质<a hidden class="anchor" aria-hidden="true" href="#第七题协方差矩阵性质">#</a></h2>
<p><strong>题目：</strong> 基于协方差矩阵定义 $\Sigma = \text{Cov}(X)$ 证明：</p>
<ol>
<li>$\Sigma$ 为对称矩阵；</li>
<li>$\Sigma$ 半正定，记 $\Sigma \geq 0$，即对任意向量 $z \in \mathbb{R}^d$ 有 $z^\top \Sigma z \geq 0$。</li>
</ol>
<p><strong>解：</strong></p>
<p>设 $X \in \mathbb{R}^d$ 为随机向量，$\mu = \mathbb{E}[X]$，则：</p>
$$\Sigma = \text{Cov}(X) = \mathbb{E}[(X-\mu)(X-\mu)^\top]$$<p><strong>(1) 证明 $\Sigma$ 为对称矩阵：</strong></p>
$$\Sigma^\top = \mathbb{E}[(X-\mu)(X-\mu)^\top]^\top = \mathbb{E}[((X-\mu)(X-\mu)^\top)^\top] = \mathbb{E}[(X-\mu)(X-\mu)^\top] = \Sigma$$<p>因此 $\Sigma$ 是对称矩阵。</p>
<p><strong>(2) 证明 $\Sigma$ 半正定：</strong></p>
<p>对任意 $z \in \mathbb{R}^d$：</p>
$$\begin{aligned} z^\top \Sigma z &= z^\top \mathbb{E}[(X-\mu)(X-\mu)^\top] z \\ &= \mathbb{E}[z^\top(X-\mu)(X-\mu)^\top z] \\ &= \mathbb{E}[(z^\top(X-\mu))^2] \\ &\geq 0 \end{aligned}$$<p>最后一步是因为期望中的项是平方项，必然非负。因此 $\Sigma$ 半正定。</p>
<hr>
<h2 id="第八题高斯判别分析的mle">第八题：高斯判别分析的MLE<a hidden class="anchor" aria-hidden="true" href="#第八题高斯判别分析的mle">#</a></h2>
<p><strong>题目：</strong> 对高斯判别分析，已知各变量概率分布为：</p>
$$\begin{aligned} p(y) &= \phi^y(1-\phi)^{1-y} \\ p(x|y=0) &= \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^\top\Sigma^{-1}(x-\mu_0)\right) \\ p(x|y=1) &= \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^\top\Sigma^{-1}(x-\mu_1)\right) \end{aligned}$$<p>证明在极大似然估计下，参数 $\phi$、$\mu_0$、$\mu_1$ 的形式为：</p>
$$\begin{aligned} \phi &= \frac{1}{n}\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=1\} \\ \mu_0 &= \frac{\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=0\}} \\ \mu_1 &= \frac{\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=1\}} \end{aligned}$$<p><strong>解：</strong></p>
<p>对数似然函数为：</p>
$$\log L = \sum_{i=1}^{n}\left[\log p(y^{(i)}) + \log p(x^{(i)}|y^{(i)})\right]$$<p><strong>估计 $\phi$：</strong></p>
$$\log L_\phi = \sum_{i=1}^{n}\log p(y^{(i)}) = \sum_{i=1}^{n}[y^{(i)}\log\phi + (1-y^{(i)})\log(1-\phi)]$$<p>令 $\frac{\partial \log L_\phi}{\partial \phi} = 0$：</p>
$$\sum_{i=1}^{n}\left[\frac{y^{(i)}}{\phi} - \frac{1-y^{(i)}}{1-\phi}\right] = 0$$<p>解得：</p>
$$\phi = \frac{1}{n}\sum_{i=1}^{n}y^{(i)} = \frac{1}{n}\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=1\}$$<p><strong>估计 $\mu_0$：</strong></p>
<p>只考虑 $y=0$ 的样本：</p>
$$\log L_{\mu_0} = \sum_{i:y^{(i)}=0}\left[-\frac{1}{2}(x^{(i)}-\mu_0)^\top\Sigma^{-1}(x^{(i)}-\mu_0) + \text{const}\right]$$<p>令 $\frac{\partial \log L_{\mu_0}}{\partial \mu_0} = 0$：</p>
$$\sum_{i:y^{(i)}=0}\Sigma^{-1}(x^{(i)}-\mu_0) = 0$$<p>解得：</p>
$$\mu_0 = \frac{\sum_{i:y^{(i)}=0}x^{(i)}}{\sum_{i:y^{(i)}=0}1} = \frac{\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^{n}\mathbb{1}\{y^{(i)}=0\}}$$<p>同理可得 $\mu_1$ 的估计。</p>
<hr>
<h2 id="第九题gda可转化为logistic回归">第九题：GDA可转化为Logistic回归<a hidden class="anchor" aria-hidden="true" href="#第九题gda可转化为logistic回归">#</a></h2>
<p><strong>题目：</strong> 证明GDA可转化为Logistic回归。提示：</p>
<ol>
<li>$p(y=1|x) = \frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=0)p(y=0)}$</li>
<li>可记 $r(x) = \frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)}$</li>
<li>给出 $p(x|y=0)$, $p(x|y=1)$, $p(y=1)$ 的表达式</li>
</ol>
<p><strong>解：</strong></p>
<p>根据贝叶斯定理：</p>
$$p(y=1|x) = \frac{p(x|y=1)p(y=1)}{p(x|y=1)p(y=1) + p(x|y=0)p(y=0)} = \frac{1}{1 + \frac{p(x|y=0)p(y=0)}{p(x|y=1)p(y=1)}} = \frac{1}{1 + \frac{1}{r(x)}}$$<p>其中：</p>
$$r(x) = \frac{p(x|y=1)p(y=1)}{p(x|y=0)p(y=0)}$$<p>计算 $\log r(x)$：</p>
$$\begin{aligned} \log r(x) &= \log p(x|y=1) + \log p(y=1) - \log p(x|y=0) - \log p(y=0) \\ &= -\frac{1}{2}(x-\mu_1)^\top\Sigma^{-1}(x-\mu_1) + \frac{1}{2}(x-\mu_0)^\top\Sigma^{-1}(x-\mu_0) + \log\frac{\phi}{1-\phi} \end{aligned}$$<p>展开：</p>
$$\begin{aligned} \log r(x) &= -\frac{1}{2}x^\top\Sigma^{-1}x + x^\top\Sigma^{-1}\mu_1 - \frac{1}{2}\mu_1^\top\Sigma^{-1}\mu_1 \\ &\quad + \frac{1}{2}x^\top\Sigma^{-1}x - x^\top\Sigma^{-1}\mu_0 + \frac{1}{2}\mu_0^\top\Sigma^{-1}\mu_0 + \log\frac{\phi}{1-\phi} \\ &= x^\top\Sigma^{-1}(\mu_1 - \mu_0) + \frac{1}{2}(\mu_0^\top\Sigma^{-1}\mu_0 - \mu_1^\top\Sigma^{-1}\mu_1) + \log\frac{\phi}{1-\phi} \\ &= \theta^\top x + \theta_0 \end{aligned}$$<p>其中：</p>
$$\theta = \Sigma^{-1}(\mu_1 - \mu_0), \quad \theta_0 = \frac{1}{2}(\mu_0^\top\Sigma^{-1}\mu_0 - \mu_1^\top\Sigma^{-1}\mu_1) + \log\frac{\phi}{1-\phi}$$<p>因此：</p>
$$p(y=1|x) = \frac{1}{1+e^{-\theta^\top x - \theta_0}} = \frac{1}{1+e^{-\tilde{\theta}^\top \tilde{x}}}$$<p>这正是Logistic回归的形式（其中 $\tilde{x}$ 包含截距项）。</p>
<hr>
<h2 id="第十题kernel-method分析">第十题：Kernel Method分析<a hidden class="anchor" aria-hidden="true" href="#第十题kernel-method分析">#</a></h2>
<p><strong>题目：</strong> Kernel method中，若Kernel function $K(x,z) = (x^\top z + c)^2$，推导对应的feature mapping $\phi$，并讨论对于 $n$ 个样本一轮SGD，使用Kernel method和在feature map上的计算效率优化比。</p>
<p>提示：</p>
<ol>
<li>基于feature map的参数更新方法为：$\theta := \theta + \alpha\sum_{i=1}^{n}(y^{(i)} - \theta^\top\phi(x^{(i)}))\phi(x^{(i)})$</li>
<li>Kernel method的参数更新方法为：$\theta := \theta + \alpha(\tilde{y} - K\theta)$，其中 $K_j = K(x^{(i)}, x^{(j)})$</li>
</ol>
<p><strong>解：</strong></p>
<p><strong>推导feature mapping：</strong></p>
<p>对于 $x,z \in \mathbb{R}^d$，展开核函数：</p>
$$\begin{aligned} K(x,z) &= (x^\top z + c)^2 \\ &= (x_1z_1 + x_2z_2 + \cdots + x_dz_d + c)^2 \\ &= \sum_{i=1}^{d}x_i^2z_i^2 + \sum_{i \neq j}2x_ix_jz_iz_j + 2c\sum_{i=1}^{d}x_iz_i + c^2 \end{aligned}$$<p>因此，feature mapping为：</p>
$$\phi(x) = (x_1^2, x_2^2, \ldots, x_d^2, \sqrt{2}x_1x_2, \sqrt{2}x_1x_3, \ldots, \sqrt{2}x_{d-1}x_d, \sqrt{2c}x_1, \ldots, \sqrt{2c}x_d, c)$$<p>维度为：$d + \binom{d}{2} + d + 1 = d + \frac{d(d-1)}{2} + d + 1 = \frac{d(d+3)}{2} + 1 = O(d^2)$</p>
<p><strong>计算效率比较：</strong></p>
<ul>
<li><strong>Feature map方法：</strong>
<ul>
<li>计算 $\phi(x^{(i)})$：$O(d^2)$ 每个样本</li>
<li>内积 $\theta^\top\phi(x^{(i)})$：$O(d^2)$</li>
<li>更新 $\theta$：$O(d^2)$</li>
<li>总计：$O(nd^2)$ 每轮SGD</li>
</ul>
</li>
<li><strong>Kernel method：</strong>
<ul>
<li>计算核矩阵 $K$：$O(n^2d)$（一次性预计算）</li>
<li>更新参数：$O(n^2)$（矩阵向量乘法）</li>
<li>总计：$O(n^2d + n^2) = O(n^2d)$ 每轮</li>
</ul>
</li>
</ul>
<p><strong>效率比：</strong></p>
$$\frac{\text{Feature map}}{\text{Kernel method}} = \frac{O(nd^2)}{O(n^2d)} = \frac{d}{n}$$<ul>
<li>当 $n \ll d$ 时（样本少，特征多），Kernel method更高效</li>
<li>当 $n \gg d$ 时（样本多，特征少），Feature map方法更高效</li>
</ul>
<hr>
<h2 id="第十一题超平面的函数间隔和几何间隔">第十一题：超平面的函数间隔和几何间隔<a hidden class="anchor" aria-hidden="true" href="#第十一题超平面的函数间隔和几何间隔">#</a></h2>
<p><strong>题目：</strong> 对超平面 $w^\top x + b = 0$，样本 $x^{(i)}$ 到的函数间隔 $\hat{\gamma}^{(i)}$ 与几何间隔 $\gamma^{(i)}$ 满足何关系？直接给出答案即可。</p>
<p><strong>解：</strong></p>
<p>函数间隔定义为：</p>
$$\hat{\gamma}^{(i)} = y^{(i)}(w^\top x^{(i)} + b)$$<p>几何间隔定义为：</p>
<p>$\gamma^{(i)} = \frac{y^{(i)}(w^\top x^{(i)} + b)}{|w|} = \frac{\hat{\gamma}^{(i)}}{|w|}$</p>
<p><strong>关系：</strong></p>
<p>$\gamma^{(i)} = \frac{\hat{\gamma}^{(i)}}{|w|}$</p>
<p>几何间隔是函数间隔除以权重向量的范数，表示点到超平面的真实距离。</p>
<h2 id="第十二题svm的lagrange函数和对偶形式">第十二题：SVM的Lagrange函数和对偶形式<a hidden class="anchor" aria-hidden="true" href="#第十二题svm的lagrange函数和对偶形式">#</a></h2>
<p><strong>题目：</strong> 已知SVM的优化目标为：</p>
$$
\min_{w,b} \quad \frac{1}{2}\|w\|^2 \qquad (1)
$$$$
\text{s.t.} \quad y^{(i)}(w^\top x^{(i)} + b) \geq 1, \quad i=1,\ldots,n
$$<p>请构造其Lagrange函数 $\mathcal{L}(w,b,\alpha)$。</p>
<p>已知 $\mathcal{L}(w,b,\alpha)$ 满足Slater条件，因此强对偶成立，问题(1)最终可转化为 $\max_{\alpha;\alpha_i\geq 0}\min_w \mathcal{L}(w,b,\alpha)$，证明该对偶形式问题可进一步转化为：</p>
$$
\max_\alpha W(\alpha) = \max_\alpha \left(\sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)}, x^{(j)}\rangle\right) \qquad (2)
$$<p>约束条件：</p>
$$
\alpha_i \geq 0, \quad i=1,\ldots,n
$$$$
\sum_{i=1}^{n}\alpha_i y^{(i)} = 0
$$<p><strong>解：</strong></p>
<p><strong>步骤1：构造Lagrange函数</strong></p>
<p>$\mathcal{L}(w,b,\alpha) = \frac{1}{2}|w|^2 - \sum_{i=1}^{n}\alpha_i[y^{(i)}(w^\top x^{(i)} + b) - 1]$</p>
<p>其中 $\alpha_i \geq 0$ 为Lagrange乘子。</p>
<p><strong>步骤2：固定 $\alpha$，对 $w$ 求导</strong></p>
<p>$\frac{\partial \mathcal{L}}{\partial w} = w - \sum_{i=1}^{n}\alpha_i y^{(i)} x^{(i)} = 0$</p>
<p>因此：</p>
<p>$w = \sum_{i=1}^{n}\alpha_i y^{(i)} x^{(i)}$</p>
<p><strong>步骤3：固定 $\alpha$，对 $b$ 求导</strong></p>
<p>$\frac{\partial \mathcal{L}}{\partial b} = -\sum_{i=1}^{n}\alpha_i y^{(i)} = 0$</p>
<p>因此：</p>
<p>$\sum_{i=1}^{n}\alpha_i y^{(i)} = 0$</p>
<p><strong>步骤4：代入Lagrange函数</strong></p>
<p>将 $w = \sum_{i=1}^{n}\alpha_i y^{(i)} x^{(i)}$ 代入 $\mathcal{L}$：</p>
<p>$\begin{aligned} \mathcal{L}(w,b,\alpha) &= \frac{1}{2}w^\top w - \sum_{i=1}^{n}\alpha_i y^{(i)} w^\top x^{(i)} - b\sum_{i=1}^{n}\alpha_i y^{(i)} + \sum_{i=1}^{n}\alpha_i \\ &= \frac{1}{2}\left(\sum_{i=1}^{n}\alpha_i y^{(i)} x^{(i)}\right)^\top\left(\sum_{j=1}^{n}\alpha_j y^{(j)} x^{(j)}\right) - \sum_{i=1}^{n}\alpha_i y^{(i)} \left(\sum_{j=1}^{n}\alpha_j y^{(j)} x^{(j)}\right)^\top x^{(i)} + \sum_{i=1}^{n}\alpha_i \\ &= \frac{1}{2}\sum_{i,j=1}^{n}\alpha_i\alpha_j y^{(i)}y^{(j)}\langle x^{(i)}, x^{(j)}\rangle - \sum_{i,j=1}^{n}\alpha_i\alpha_j y^{(i)}y^{(j)}\langle x^{(i)}, x^{(j)}\rangle + \sum_{i=1}^{n}\alpha_i \\ &= \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{n}\alpha_i\alpha_j y^{(i)}y^{(j)}\langle x^{(i)}, x^{(j)}\rangle \end{aligned}$</p>
<p>其中使用了 $\sum_{i=1}^{n}\alpha_i y^{(i)} = 0$，所以 $b$ 项消失。</p>
<p>因此对偶问题为：</p>
<p>$\max_\alpha W(\alpha) = \max_\alpha \left(\sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)}, x^{(j)}\rangle\right)$</p>
<p>约束条件为：</p>
<p>$\begin{aligned} \alpha_i &\geq 0, \quad i=1,\ldots,n \\ \sum_{i=1}^{n}\alpha_i y^{(i)} &= 0 \end{aligned}$</p>
<hr>
<h2 id="第十三题线性不可分的svm与l1正则">第十三题：线性不可分的SVM与L1正则<a hidden class="anchor" aria-hidden="true" href="#第十三题线性不可分的svm与l1正则">#</a></h2>
<p><strong>题目：</strong> 对线性不可分的训练集，SVM对应带L1正则的优化目标是什么？已知对线性可分情况的优化为：</p>
$$ \min_{w,b} \quad \frac{1}{2}\|w\|^2 \qquad (1) $$<p> </p>
$$ \text{s.t.} \quad y^{(i)}(w^\top x^{(i)} + b) \geq 1, \quad i=1,\ldots,n $$<p><strong>解：</strong></p>
<p>对于线性不可分的情况，引入松弛变量 $\xi_i \geq 0$，允许某些样本违反间隔约束。</p>
<p><strong>带L1正则的软间隔SVM优化目标为：</strong></p>
<p>$\begin{aligned} \min_{w,b,\xi} &\quad \frac{1}{2}|w|^2 + C\sum_{i=1}^{n}\xi_i \\ \text{s.t.} &\quad y^{(i)}(w^\top x^{(i)} + b) \geq 1 - \xi_i, \quad i=1,\ldots,n \\ &\quad \xi_i \geq 0, \quad i=1,\ldots,n \end{aligned}$</p>
<p>其中：</p>
<ul>
<li>$\xi_i$ 是松弛变量，表示样本 $i$ 违反间隔的程度</li>
<li>$C > 0$ 是惩罚参数，控制间隔最大化与违反程度之间的权衡</li>
<li>$C\sum_{i=1}^{n}\xi_i$ 是L1正则项（对松弛变量的惩罚）</li>
</ul>
<p>这个目标函数平衡了两个目标：</p>
<ol>
<li>最大化间隔（通过最小化 $|w|^2$）</li>
<li>最小化分类错误（通过最小化 $\sum\xi_i$）</li>
</ol>
<hr>
<h2 id="第十四题svm最优化问题分析">第十四题：SVM最优化问题分析<a hidden class="anchor" aria-hidden="true" href="#第十四题svm最优化问题分析">#</a></h2>
<p><strong>题目：</strong> 已知SVM的最终优化目标为：</p>
<p>$W(\alpha) = \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{n}y^{(i)}y^{(j)}\alpha_i\alpha_j\langle x^{(i)}, x^{(j)}\rangle$</p>
<p>假设此时正在优化 $\alpha_1$ 与 $\alpha_2$，并有 $\alpha_1 = (\zeta - \alpha_2y^{(2)})y^{(1)}$。请推导此时 $\alpha_2$ 应当更新的值。</p>
<p><strong>解：</strong></p>
<p><strong>简化目标函数：</strong></p>
<p>在固定其他 $\alpha_i$ ($i \geq 3$) 的情况下，目标函数关于 $\alpha_1, \alpha_2$ 可写为：</p>
<p>$W(\alpha_1, \alpha_2) = \alpha_1 + \alpha_2 + W_0 - \frac{1}{2}[K_{11}\alpha_1^2 + K_{22}\alpha_2^2 + 2K_{12}\alpha_1\alpha_2y^{(1)}y^{(2)}] + \text{线性项}$</p>
<p>其中 $K_{ij} = \langle x^{(i)}, x^{(j)}\rangle$，$W_0$ 是常数项。</p>
<p><strong>利用约束 $\alpha_1 = (\zeta - \alpha_2y^{(2)})y^{(1)}$：</strong></p>
<p>这个约束来自 $\sum_{i=1}^{n}\alpha_i y^{(i)} = 0$，可以改写为：</p>
<p>$\alpha_1 y^{(1)} + \alpha_2 y^{(2)} = -\sum_{i=3}^{n}\alpha_i y^{(i)} = \zeta \quad \text{（常数）}$</p>
<p>将 $\alpha_1 = (\zeta - \alpha_2y^{(2)})y^{(1)}$ 代入目标函数，得到关于 $\alpha_2$ 的单变量优化问题。</p>
<p>对 $\alpha_2$ 求导并令其为0，经过复杂推导（涉及预测误差），得到 $\alpha_2$ 的无约束最优解：</p>
<p>$\alpha_2^{\text{new, unc}} = \alpha_2^{\text{old}} + \frac{y^{(2)}(E_1 - E_2)}{\eta}$</p>
<p>其中：</p>
<ul>
<li>$E_i = f(x^{(i)}) - y^{(i)}$ 是预测误差</li>
<li>$\eta = K_{11} + K_{22} - 2K_{12} = \|x^{(1)} - x^{(2)}\|^2$（特征空间距离）</li>
</ul>
<p><strong>考虑约束 $0 \leq \alpha_2 \leq C$：</strong></p>
<p>根据约束 $\alpha_1 y^{(1)} + \alpha_2 y^{(2)} = \zeta$：</p>
<ul>
<li>若 $y^{(1)} \neq y^{(2)}$： $L = \max(0, \alpha_2^{\text{old}} - \alpha_1^{\text{old}}), \quad H = \min(C, C + \alpha_2^{\text{old}} - \alpha_1^{\text{old}})$</li>
<li>若 $y^{(1)} = y^{(2)}$： $L = \max(0, \alpha_1^{\text{old}} + \alpha_2^{\text{old}} - C), \quad H = \min(C, \alpha_1^{\text{old}} + \alpha_2^{\text{old}})$</li>
</ul>
<p><strong>最终更新公式：</strong></p>
<p>$\alpha_2^{\text{new}} = \begin{cases} H & \text{if } \alpha_2^{\text{new, unc}} > H \\ \alpha_2^{\text{new, unc}} & \text{if } L \leq \alpha_2^{\text{new, unc}} \leq H \\ L & \text{if } \alpha_2^{\text{new, unc}} < L \end{cases}$</p>
<p>然后通过约束更新 $\alpha_1$：</p>
<p>$\alpha_1^{\text{new}} = \alpha_1^{\text{old}} + y^{(1)}y^{(2)}(\alpha_2^{\text{old}} - \alpha_2^{\text{new}})$</p>
<hr>
<h2 id="第十五题信息增益比计算">第十五题：信息增益比计算<a hidden class="anchor" aria-hidden="true" href="#第十五题信息增益比计算">#</a></h2>
<p><strong>题目：</strong> 计算给定数据集中四个特征的信息增益比。可保留log项，统一底数为2。</p>
<p><a href="https://postimg.cc/N9Bhq9Cf"><img alt="image.png" loading="lazy" src="https://i.postimg.cc/Px8dD1mZ/image.png"></a></p>
<p><strong>解：</strong></p>
<p>首先计算数据集的熵。类别分布：否=6，是=9，总计15。</p>
<p>$H(D) = -\frac{6}{15}\log_2\frac{6}{15} - \frac{9}{15}\log_2\frac{9}{15} = -0.4\log_2(0.4) - 0.6\log_2(0.6) = 0.971$</p>
<h3 id="特征1年龄">特征1：年龄<a hidden class="anchor" aria-hidden="true" href="#特征1年龄">#</a></h3>
<ul>
<li>青年(5个)：否=3，是=2，$H = 0.971$</li>
<li>中年(5个)：否=1，是=4，$H = 0.722$</li>
<li>老年(5个)：否=2，是=3，$H = 0.971$</li>
</ul>
<p>条件熵：</p>
<p>$H(D|\text{年龄}) = \frac{5}{15}(0.971) + \frac{5}{15}(0.722) + \frac{5}{15}(0.971) = 0.888$</p>
<p>信息增益：</p>
<p>$\text{Gain}(\text{年龄}) = 0.971 - 0.888 = 0.083$</p>
<p>特征熵（分裂信息）：</p>
<p>$H_A(\text{年龄}) = -3 \times \frac{5}{15}\log_2\frac{5}{15} = \log_2 3 = 1.585$</p>
<p>信息增益比：</p>
$$ \text{Gain\_ratio}(\text{年龄}) = \frac{0.083}{1.585} = 0.052 $$<h3 id="特征2有工作">特征2：有工作<a hidden class="anchor" aria-hidden="true" href="#特征2有工作">#</a></h3>
<ul>
<li>否(8个)：否=4，是=4，$H = 1.0$</li>
<li>是(7个)：否=2，是=5，$H = 0.863$</li>
</ul>
<p>条件熵：$H(D|\text{有工作}) = 0.936$</p>
<p>信息增益：$\text{Gain}(\text{有工作}) = 0.035$</p>
<p>特征熵：$H_A(\text{有工作}) = 0.997$</p>
<p>信息增益比：</p>
$$ \text{Gain\_ratio}(\text{有工作}) = 0.035 $$<h3 id="特征3有自己的房子">特征3：有自己的房子<a hidden class="anchor" aria-hidden="true" href="#特征3有自己的房子">#</a></h3>
<ul>
<li>否(9个)：否=3，是=6，$H = 0.918$</li>
<li>是(6个)：否=3，是=3，$H = 1.0$</li>
</ul>
<p>条件熵：$H(D|\text{有房}) = 0.951$</p>
<p>信息增益：$\text{Gain}(\text{有房}) = 0.020$</p>
<p>特征熵：$H_A(\text{有房}) = 0.971$</p>
<p>信息增益比：</p>
$$ \text{Gain\_ratio}(\text{有房}) = 0.021 $$<h3 id="特征4信贷情况">特征4：信贷情况<a hidden class="anchor" aria-hidden="true" href="#特征4信贷情况">#</a></h3>
<ul>
<li>一般(5个)：否=4，是=1，$H = 0.722$</li>
<li>好(6个)：否=2，是=4，$H = 0.918$</li>
<li>非常好(4个)：否=0，是=4，$H = 0$</li>
</ul>
<p>条件熵：$H(D|\text{信贷}) = 0.608$</p>
<p>信息增益：$\text{Gain}(\text{信贷}) = 0.363$</p>
<p>特征熵：$H_A(\text{信贷}) = 1.557$</p>
<p>信息增益比：</p>
$$ \text{Gain\_ratio}(\text{信贷}) = 0.233 $$<h3 id="总结信息增益比排序">总结（信息增益比排序）<a hidden class="anchor" aria-hidden="true" href="#总结信息增益比排序">#</a></h3>
<ol>
<li><strong>信贷情况：0.233</strong> ⭐（最佳分裂特征）</li>
<li>年龄：0.052</li>
<li>有工作：0.035</li>
<li>有自己的房子：0.021</li>
</ol>
<p>应选择&quot;信贷情况&quot;作为根节点的分裂特征。</p>
<hr>
<h2 id="第十六题xgboost损失函数二阶泰勒展开">第十六题：XGBoost损失函数二阶泰勒展开<a hidden class="anchor" aria-hidden="true" href="#第十六题xgboost损失函数二阶泰勒展开">#</a></h2>
<p><strong>题目：</strong> 已知XGBoost优化第t棵树时的损失函数为：</p>
<p>$\mathcal{L}^{(t)} = \sum_{i=1}^{n}l(y_i, \hat{y}*i^{(t-1)} + f_t(x_i)) + \gamma T + \frac{1}{2}\lambda\sum*{j=1}^{T}w_j^2$</p>
<p>请推导 $l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i))$ 在 $l(y_i, \hat{y}_i^{(t-1)})$ 处对于 $f_t(x_i)$ 的二阶泰勒展开。其中，一阶和二阶导数可使用：</p>
<p>$g_i = \frac{\partial l(y_i, \hat{y}_i)}{\partial \hat{y}*i}\Big|*{\hat{y}_i^{(t-1)}}, \quad h_i = \frac{\partial^2 l(y_i, \hat{y}_i)}{\partial \hat{y}*i^2}\Big|*{\hat{y}_i^{(t-1)}}$</p>
<p>在此基础上，推导叶子节点 $j$ 对应的 $w_j^*$ 满足：</p>
<p>$w_j^* = -\frac{\sum_{i \in \mathcal{I}*j}g_i}{\sum*{i \in \mathcal{I}_j}h_i + \lambda}$</p>
<p>其中，$\mathcal{I}_j = \{i \mid q(x_i) = j\}$ 表示属于叶子节点 $j$ 的样本集合。</p>
<p><strong>解：</strong></p>
<h3 id="二阶泰勒展开">二阶泰勒展开<a hidden class="anchor" aria-hidden="true" href="#二阶泰勒展开">#</a></h3>
<p>在 $\hat{y}_i^{(t-1)}$ 处对 $l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i))$ 关于 $f_t(x_i)$ 进行二阶泰勒展开：</p>
<p>$\begin{aligned} l(y_i, \hat{y}_i^{(t-1)} + f_t(x_i)) &\approx l(y_i, \hat{y}_i^{(t-1)}) + \frac{\partial l(y_i, \hat{y}_i)}{\partial \hat{y}*i}\Big|*{\hat{y}_i^{(t-1)}} \cdot f_t(x_i) \\ &\quad + \frac{1}{2}\frac{\partial^2 l(y_i, \hat{y}_i)}{\partial \hat{y}*i^2}\Big|*{\hat{y}_i^{(t-1)}} \cdot f_t(x_i)^2 \\ &= l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2}h_i f_t(x_i)^2 \end{aligned}$</p>
<p>因此损失函数变为：</p>
<p>$\mathcal{L}^{(t)} \approx \sum_{i=1}^{n}[l(y_i, \hat{y}*i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2}h_i f_t(x_i)^2] + \gamma T + \frac{1}{2}\lambda\sum*{j=1}^{T}w_j^2$</p>
<p>去掉常数项 $\sum_{i=1}^{n}l(y_i, \hat{y}_i^{(t-1)})$：</p>
<p>$\tilde{\mathcal{L}}^{(t)} = \sum_{i=1}^{n}[g_i f_t(x_i) + \frac{1}{2}h_i f_t(x_i)^2] + \gamma T + \frac{1}{2}\lambda\sum_{j=1}^{T}w_j^2$</p>
<h3 id="推导叶子权重">推导叶子权重 $w_j^*$<a hidden class="anchor" aria-hidden="true" href="#推导叶子权重">#</a></h3>
<p>对于树模型，$f_t(x_i) = w_{q(x_i)}$，其中 $q(x_i)$ 表示样本 $i$ 落在的叶子节点。</p>
<p>将样本按叶子节点分组：</p>
<p>$\tilde{\mathcal{L}}^{(t)} = \sum_{j=1}^{T}\left[\left(\sum_{i \in \mathcal{I}*j}g_i\right)w_j + \frac{1}{2}\left(\sum*{i \in \mathcal{I}_j}h_i + \lambda\right)w_j^2\right] + \gamma T$</p>
<p>记 $G_j = \sum_{i \in \mathcal{I}*j}g_i$，$H_j = \sum*{i \in \mathcal{I}_j}h_i$，则：</p>
<p>$\tilde{\mathcal{L}}^{(t)} = \sum_{j=1}^{T}\left[G_j w_j + \frac{1}{2}(H_j + \lambda)w_j^2\right] + \gamma T$</p>
<p>对 $w_j$ 求导并令其为0：</p>
<p>$\frac{\partial \tilde{\mathcal{L}}^{(t)}}{\partial w_j} = G_j + (H_j + \lambda)w_j = 0$</p>
<p>解得：</p>
<p>$w_j^* = -\frac{G_j}{H_j + \lambda} = -\frac{\sum_{i \in \mathcal{I}*j}g_i}{\sum*{i \in \mathcal{I}_j}h_i + \lambda}$</p>
<p>这就是叶子节点的最优权重。将其代入损失函数，得到：</p>
<p>$\tilde{\mathcal{L}}^{(t)} = -\frac{1}{2}\sum_{j=1}^{T}\frac{G_j^2}{H_j + \lambda} + \gamma T$</p>
<p>这个公式用于评估树结构的质量，指导分裂决策。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></li>
      <li><a href="http://localhost:1313/tags/%E6%95%B0%E5%AD%A6%E6%8E%A8%E5%AF%BC/">数学推导</a></li>
      <li><a href="http://localhost:1313/tags/%E7%AE%97%E6%B3%95/">算法</a></li>
    </ul>
  </footer><div id="tw-comment"></div>
<script>
    
    const getStoredTheme = () => localStorage.getItem("pref-theme") === "light" ? "nord_light" : "dark_dimmed";
    const setGiscusTheme = () => {
        const sendMessage = (message) => {
            const iframe = document.querySelector('iframe.giscus-frame');
            if (iframe) {
                iframe.contentWindow.postMessage({giscus: message}, 'https://giscus.app');
            }
        }
        sendMessage({setConfig: {theme: getStoredTheme()}})
    }

    document.addEventListener("DOMContentLoaded", () => {
        const giscusAttributes = {
            "src": "https://giscus.app/client.js",
            "data-repo": "minjieblog\/minjieblog.github.io",
            "data-repo-id": "R_kgDOQjMm6A",
            "data-category": "Announcements",
            "data-category-id": "DIC_kwDOQjMm6M4Czb4B",
            "data-mapping": "pathname",
            "data-strict": "0",
            "data-reactions-enabled": "1",
            "data-emit-metadata": "0",
            "data-input-position": "bottom",
            "data-theme": getStoredTheme(),
            "data-lang": "zh-CN",
            "data-loading": "lazy",
            "crossorigin": "anonymous",
        };

        
        const giscusScript = document.createElement("script");
        Object.entries(giscusAttributes).forEach(
                ([key, value]) => giscusScript.setAttribute(key, value));
        document.querySelector("#tw-comment").appendChild(giscusScript);

        
        const themeSwitcher = document.querySelector("#theme-toggle");
        if (themeSwitcher) {
            themeSwitcher.addEventListener("click", setGiscusTheme);
        }
        const themeFloatSwitcher = document.querySelector("#theme-toggle-float");
        if (themeFloatSwitcher) {
            themeFloatSwitcher.addEventListener("click", setGiscusTheme);
        }
    });
</script>
</article>
    </main>
    
<footer class="footer">
        <span><a href="https://minjieblog.github.io/">©2025 Minjie&rsquo;s Blog</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu');
    if (menu) {
        
        const scrollPosition = localStorage.getItem("menu-scroll-position");
        if (scrollPosition) {
            menu.scrollLeft = parseInt(scrollPosition, 10);
        }
        
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        const html = document.querySelector("html");
        if (html.dataset.theme === "dark") {
            html.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            html.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = '复制';

        function copyingDone() {
            copybutton.innerHTML = '已复制！';
            setTimeout(() => {
                copybutton.innerHTML = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
