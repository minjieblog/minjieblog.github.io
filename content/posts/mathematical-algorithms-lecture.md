---
title: "数学基础算法篇"
date: 2025-12-21T15:50:54+08:00
draft: false
tags: ["算法", "哈希", "算法分析", "尾不等式", "数据流"]
categories: ["研究生课程"]
summary: |
    本笔记涵盖数学基础算法篇的完整内容，包括算法分析与相似度搜索、尾不等式及其应用、数据流算法（频繁元素检测、滑动窗口模型）、分布式数据流处理、哈希技术（布隆过滤器、LSH）、线性规划与整数规划、内存计算架构、社区发现算法以及子模函数应用等核心理论与实践。
description: |
    金老师主讲的研究生课程《数学基础算法篇》完整笔记，从算法分析基础开始，系统介绍相似度搜索、概率不等式、流式数据处理、分布式计算、哈希技术、优化算法、图算法等内容，理论与实践相结合，为解决大规模数据处理问题提供数学和算法基础。
comments: true
---

## 第一讲 算法分析

### 一、算法简介

算法分析是计算机科学的基础，用于评估算法的效率和资源消耗。

**算法复杂度分析：**
- **时间复杂度**：衡量算法执行时间随输入规模增长的趋势
- **空间复杂度**：衡量算法所需存储空间随输入规模增长的趋势
- **渐近分析**：O、Ω、Θ记号

**常见复杂度类型：**
- O(1)：常数时间
- O(log n)：对数时间
- O(n)：线性时间
- O(n log n)：线性对数时间
- O(n²)：平方时间
- O(2ⁿ)：指数时间

**算法设计范式：**
- 分治法（Divide and Conquer）
- 动态规划（Dynamic Programming）
- 贪心算法（Greedy Algorithm）
- 回溯法（Backtracking）
- 分支界定法（Branch and Bound）

### 二、相似度搜索

相似度搜索是在大规模数据中找到与查询对象相似的数据项。

**相似度度量：**
- **欧氏距离（Euclidean Distance）**：
  ```
  d(x, y) = √(Σ(xᵢ - yᵢ)²)
  ```
- **曼哈顿距离（Manhattan Distance）**：
  ```
  d(x, y) = Σ|xᵢ - yᵢ|
  ```
- **余弦相似度（Cosine Similarity）**：
  ```
  sim(x, y) = (x·y) / (||x|| ||y||)
  ```
- **Jaccard相似度**：
  ```
  J(A, B) = |A ∩ B| / |A ∪ B|
  ```

**应用场景：**
- 推荐系统
- 图像检索
- 文档相似度检测
- 近似最近邻搜索（ANN）

**朴素方法的局限：**
线性扫描的时间复杂度为O(n)，对于大规模数据集不可行。

### 三、高维空间的相似性搜索

在高维空间中，传统的索引结构（如KD树）性能退化严重，这被称为"维度灾难"。

**维度灾难（Curse of Dimensionality）：**
- 数据点之间的距离趋于相等
- 数据点趋向于分布在空间的"表面"
- 索引结构的效率降低

**解决方案：**

**1. 降维技术：**
- 主成分分析（PCA）
- 随机投影
- t-SNE

**2. 近似算法：**
- **局部敏感哈希（LSH）**：将相似的点以高概率映射到同一个桶
- **乘积量化（Product Quantization）**：将向量空间分解为子空间
- **HNSW（Hierarchical Navigable Small World）**：基于图的近似最近邻搜索

**LSH的基本原理：**
- 使用哈希函数族h，满足：相似的对象碰撞概率高，不相似的对象碰撞概率低
- 通过多个哈希函数和多个哈希表提高召回率
- 查询时只需检查哈希到同一桶的候选点

---

## 第二讲 尾不等式

### 一、尾不等式分析概要

尾不等式用于界定随机变量偏离其期望值的概率，在算法分析和概率算法设计中具有重要作用。

**为什么需要尾不等式：**
- 期望值只描述了平均情况
- 需要了解"坏情况"发生的概率
- 为随机算法提供性能保证

**应用领域：**
- 随机算法分析
- 负载均衡
- 哈希表性能分析
- 采样算法

### 二、三个常见的尾不等式

**1. Markov不等式（马尔可夫不等式）：**

对于非负随机变量X和a > 0：
```
P(X ≥ a) ≤ E[X] / a
```

**特点：**
- 只需要知道期望
- 界比较松
- 适用于任何非负随机变量

**2. Chebyshev不等式（切比雪夫不等式）：**

对于随机变量X，有：
```
P(|X - E[X]| ≥ a) ≤ Var(X) / a²
```

**特点：**
- 需要知道期望和方差
- 比Markov不等式更紧
- 不需要独立性假设

**3. Chernoff界（切尔诺夫界）：**

对于独立的随机变量X₁, X₂, ..., Xₙ ∈ [0,1]，令X = ΣXᵢ，μ = E[X]：
```
P(X ≥ (1+δ)μ) ≤ e^(-δ²μ/3)  (0 < δ ≤ 1)
P(X ≤ (1-δ)μ) ≤ e^(-δ²μ/2)  (0 < δ ≤ 1)
```

**特点：**
- 界最紧，指数级衰减
- 需要独立性假设
- 广泛应用于随机算法分析

### 三、计数问题

**问题描述：**
估计一个数据流中不同元素的个数（基数估计）。

**朴素方法的问题：**
- 存储所有元素需要O(n)空间
- 对于大规模数据流不可行

**概率计数算法：**

**Flajolet-Martin算法：**
1. 使用哈希函数h将元素映射到[0, 2^L-1]
2. 对每个元素计算其哈希值的尾部0的个数
3. 记录观察到的最大尾部0个数R
4. 估计值：2^R

**原理：**
- 如果有n个不同元素，期望看到log₂(n)个尾部0
- 使用多个哈希函数取平均值提高精度
- 空间复杂度：O(log log n)

**HyperLogLog算法：**
- Flajolet-Martin的改进版本
- 使用调和平均数代替算术平均数
- 精度更高，误差约为1.04/√m（m为桶数）
- 空间效率极高

---

## 第三讲 数据流

### 一、数据流模型

数据流模型描述了数据以流的形式到达，只能顺序访问一次或有限次的计算场景。

**数据流的特点：**
- 数据量巨大，无法全部存储
- 数据到达速度快，需要实时处理
- 只能进行一次或有限次扫描
- 需要在有限空间内近似计算

**典型应用：**
- 网络流量监控
- 金融交易分析
- 社交媒体数据处理
- IoT传感器数据

**数据流算法的目标：**
- 使用亚线性（sublinear）空间
- 提供可证明的近似保证
- 单次扫描或少量扫描

### 二、频繁元素-确定性算法

**问题定义：**
找出数据流中出现频率超过某个阈值的元素（heavy hitters）。

**Misra-Gries算法：**

**算法描述：**
1. 维护最多k个计数器
2. 对于每个到来的元素：
   - 如果已有计数器，增加其计数
   - 如果没有计数器且有空位，创建新计数器
   - 如果没有空位，所有计数器减1，删除值为0的计数器
3. 输出计数器中的元素

**性能保证：**
- 空间复杂度：O(k)
- 如果元素出现次数 > n/k，一定会被找到
- 可能有假阳性，但可以通过二次扫描验证

**应用：**
- 找出访问量最大的网页
- 检测网络中的大流量
- 识别热门话题

### 三、频繁元素-随机算法

**Count-Min Sketch算法：**

**数据结构：**
- d × w 的二维计数器数组
- d个独立的哈希函数h₁, h₂, ..., h_d

**算法操作：**

**更新（插入元素x）：**
```
对于 i = 1 到 d：
    count[i][hᵢ(x)] += 1
```

**查询（估计元素x的频率）：**
```
返回 min{count[i][hᵢ(x)] : i = 1..d}
```

**性能保证：**
- 空间复杂度：O(d × w)
- 误差界：ε·n (n为总元素数)，概率至少1-δ
- 选择w = ⌈e/ε⌉, d = ⌈ln(1/δ)⌉
- 只会高估，不会低估

**优势：**
- 空间效率高
- 支持点查询和范围查询
- 可以处理删除操作（使用带符号的计数）

### 四、滑动窗口模型

**问题描述：**
只关心最近W个元素的统计特性，更早的数据被丢弃。

**挑战：**
- 无法存储所有W个元素
- 需要及时更新统计信息

**DGIM算法（用于计数）：**

**基本思想：**
- 将窗口划分为桶（bucket）
- 每个桶代表一段连续的1
- 桶的大小是2的幂次
- 维护O(log W)个桶

**桶的性质：**
1. 每种大小的桶最多2个
2. 桶按时间戳排序
3. 最老的桶可能不完整

**查询操作：**
统计窗口内1的个数 ≈ 完整桶的大小之和 + 半个最老桶

**误差保证：**
- 相对误差：最多50%
- 可以通过增加每种大小桶的数量来降低误差

**应用扩展：**
- 滑动窗口中的平均值
- 滑动窗口中的中位数（近似）
- 滑动窗口中的distinct计数

---

## 第四讲 分布式数据流

### 一、分布式数据流模型

在分布式环境中，数据流分散在多个节点上，需要协调多个节点进行计算。

**系统架构：**
- **多个监测节点**：每个节点观察部分数据流
- **协调节点**：汇总和处理来自监测节点的信息
- **通信约束**：最小化节点间的通信量

**挑战：**
- 数据分散性
- 通信开销
- 同步问题
- 节点故障

**典型场景：**
- 分布式网络监控
- 多数据中心的日志分析
- 边缘计算
- CDN流量统计

### 二、聚集查询

**问题定义：**
计算分布在多个节点上的数据的聚集函数（如SUM、COUNT、AVG）。

**基本方法：**

**1. 连续聚集：**
- 每个节点维护本地的统计信息
- 周期性发送给协调节点
- 协调节点汇总计算全局结果

**2. 快照聚集：**
- 在特定时刻获取全局快照
- 需要处理同步问题
- 使用逻辑时钟或物理时钟

**优化技术：**

**采样与估计：**
- 不发送所有数据，只发送样本
- 使用统计方法估计全局结果
- 权衡精度和通信开销

**增量更新：**
- 只发送变化部分
- 减少冗余通信
- 适用于变化缓慢的数据

**数据结构支持：**
- 使用Count-Min Sketch等概要结构
- 可以在协调节点合并
- 支持分布式查询

### 三、topk监控

**问题定义：**
实时监控分布式系统中全局的top-k元素（如最热门的k个商品、最活跃的k个用户）。

**挑战：**
- 全局top-k可能不在任何单个节点的局部top-k中
- 需要在精度和通信量之间平衡
- 数据分布可能高度倾斜

**解决方案：**

**1. 阈值算法：**
- 协调节点维护全局top-k的阈值θ
- 每个节点报告超过θ的元素
- 动态调整θ以平衡通信量

**算法流程：**
```
初始化：θ = 0
循环：
  1. 每个节点报告频率 > θ 的元素
  2. 协调节点更新全局top-k
  3. 计算新阈值θ（如第k大元素的频率）
  4. 将θ广播给各节点
```

**2. 采样方法：**
- 各节点以概率p采样元素
- 上传采样的数据到协调节点
- 基于采样数据估计全局top-k

**3. 层次化监控：**
- 构建监控树
- 中间节点聚合子节点的信息
- 减少单点通信压力

**性能优化：**
- **局部过滤**：只上传可能进入全局top-k的元素
- **批量通信**：积累一定数量的更新后批量发送
- **缓存机制**：利用时间局部性减少通信

**实际应用：**
- 实时热搜榜
- 分布式缓存的热点识别
- 网络安全中的异常检测
- 广告系统的CTR监控

---

## 第五讲 哈希

### 一、哈希函数和哈希表

哈希技术是一种通过哈希函数将数据映射到固定大小的表中的方法，实现快速的数据存储和检索。

**核心概念：**
- **哈希函数**：将任意大小的数据映射到固定大小的值
- **哈希表**：基于数组实现的数据结构，通过哈希函数计算索引位置
- **冲突处理**：当不同的键映射到相同位置时的解决策略
  - 链地址法（Chaining）
  - 开放地址法（Open Addressing）

**时间复杂度：**
- 平均情况：O(1) 查找、插入、删除
- 最坏情况：O(n)（当所有元素都冲突时）

### 二、布隆过滤器（Bloom Filter）

布隆过滤器是一种空间高效的概率型数据结构，用于判断一个元素是否在集合中。

**特点：**
- 可能产生假阳性（False Positive）：说存在但实际不存在
- 不会产生假阴性（False Negative）：说不存在就一定不存在
- 不支持删除操作（标准版本）

**应用场景：**
- 网页URL去重
- 垃圾邮件过滤
- 缓存穿透防护
- 大数据去重

**工作原理：**
1. 使用k个不同的哈希函数
2. 将元素映射到位数组的k个位置
3. 查询时检查这k个位置是否都为1

### 三、最小哈希和LSH（Locality-Sensitive Hashing）

**最小哈希（MinHash）：**
用于估计两个集合的Jaccard相似度，常用于文档去重和相似度检测。

**局部敏感哈希（LSH）：**
一种降维技术，使得相似的数据项以高概率被映射到相同的桶中。

**应用：**
- 近似最近邻搜索
- 图像相似度检测
- 文本去重
- 推荐系统

---

## 第六讲 线性规划与整数规划

### 一、线性规划：单纯形算法

**线性规划问题标准形式：**
```
最大化/最小化：c^T x
约束条件：Ax ≤ b, x ≥ 0
```

**单纯形算法（Simplex Method）：**
- 由George Dantzig于1947年提出
- 从可行域的一个顶点出发
- 沿着目标函数值改进的方向移动到相邻顶点
- 直到找到最优解或判定无界

**算法步骤：**
1. 将问题转换为标准形式
2. 找到初始基本可行解
3. 检验数判断是否达到最优
4. 若未达到最优，选择入基变量和出基变量
5. 进行基变换，更新解
6. 重复步骤3-5直到最优

### 二、整数规划：问题定义

整数规划是线性规划的扩展，要求部分或全部变量取整数值。

**分类：**
- **纯整数规划**：所有变量都必须是整数
- **混合整数规划（MIP）**：部分变量是整数，部分是连续变量
- **0-1整数规划**：变量只能取0或1

**应用场景：**
- 资源分配
- 生产计划
- 网络设计
- 任务调度

### 三、整数规划：分支界定法（Branch and Bound）

**基本思想：**
将原问题分解为若干子问题，通过界定（bounding）技术减少搜索空间。

**算法流程：**
1. **松弛**：求解去掉整数约束的线性规划松弛问题
2. **界定**：用松弛问题的最优值作为上界（最大化问题）
3. **分支**：选择一个非整数变量进行分支
4. **剪枝**：
   - 当前节点的界不如已知最优解时剪枝
   - 松弛问题无可行解时剪枝
   - 松弛问题的解满足整数约束时更新最优解

### 四、整数规划：切平面法（Cutting Plane Method）

**核心思想：**
通过添加线性约束（切平面）逐步逼近整数规划的可行域，直到得到整数最优解。

**Gomory割平面：**
- 从单纯形表中推导出的割平面
- 切掉当前非整数最优解
- 不切掉任何整数可行解

**算法步骤：**
1. 求解线性规划松弛问题
2. 若解为整数，则为最优解，算法结束
3. 若解不为整数，生成割平面约束
4. 将割平面加入原问题
5. 重新求解并重复步骤2-4

---

## 第七讲 内存计算

### 一、海量内存概述

随着硬件技术的发展，单机内存容量已经可以达到TB级别，这为大数据处理带来了新的机遇。

**内存计算的优势：**
- **速度快**：内存访问速度比磁盘快3-5个数量级
- **延迟低**：避免了磁盘I/O的延迟
- **适合迭代计算**：机器学习、图计算等需要多次迭代的场景

**挑战：**
- 成本较高
- 数据持久化问题
- 容错机制

### 二、基于单机版内存增大优势

**单机大内存的应用场景：**
- 内存数据库（如Redis, Memcached）
- 实时数据分析
- 高性能缓存系统
- 图数据处理

**技术特点：**
- 简化系统架构，减少网络通信开销
- 提高数据处理的吞吐量
- 适合中等规模的数据集

**优化策略：**
- 数据结构优化（压缩、列式存储）
- NUMA-aware内存分配
- 大页内存（Huge Pages）的使用

### 三、基于共享式内存和分布式内存结合架构优势

**混合架构设计：**

**共享式内存（Shared Memory）：**
- 多个处理器共享同一物理内存空间
- 通信开销低
- 编程相对简单
- 可扩展性受限

**分布式内存（Distributed Memory）：**
- 每个节点有独立的内存空间
- 通过网络进行通信
- 可扩展性强
- 编程复杂度较高

**结合架构的优势：**
1. **弹性扩展**：根据数据规模动态调整资源
2. **负载均衡**：合理分配计算任务
3. **容错能力**：数据副本和快照机制
4. **性能优化**：本地内存访问 + 跨节点通信

**典型系统：**
- Apache Spark（基于RDD的内存计算框架）
- Apache Flink（流式和批处理统一计算）
- Alluxio（分布式内存文件系统）

### 四、总结

内存计算已成为大数据处理的重要技术方向：
- 针对不同场景选择合适的架构
- 平衡性能、成本和可扩展性
- 结合持久化存储保证数据安全
- 关注新硬件技术（如持久化内存）的发展

---

## 第八讲 社区发现

社区发现是复杂网络分析中的重要问题，目标是找出网络中紧密连接的节点群组。

### 一、图切割（Graph Partitioning）

#### 1. 社区划分问题

给定无向图 $G = (V, E)$，其中：
- $V$ 表示所有的顶点（节点）集合
- $E$ 表示所有的边集合

**任务：** 将所有顶点分成两个不相交的组：
- 组 $A$：包含一部分节点
- 组 $B = V\backslash A$：包含剩余的所有节点（即 $V$ 中除了 $A$ 之外的所有节点）

**核心问题：** 如何评判这个划分的好坏？

#### 2. 评判准则

一个良好的社区划分应该满足：

- **最大化社区内部的连接数**：同一个社区内的节点之间应该有尽可能多的边连接
- **最小化社区之间的连接数**：不同社区之间的连接应该尽可能少

#### 3. 割（Cut）的定义

为了量化划分的质量，我们引入"割"的概念。

**割(cut)** 是指：只有一个端点在社区 $A$ 内，另一个端点在社区 $A$ 外的所有边的权重之和。

数学表达式：

$$cut(A) = \sum_{i \in A, j \notin A} w_{ij}$$

**公式解释：**
- $i \in A$：节点 $i$ 在社区 $A$ 中
- $j \notin A$：节点 $j$ 不在社区 $A$ 中（即在社区 $B$ 中）
- $w_{ij}$：连接节点 $i$ 和节点 $j$ 的边的权重（如果是无权图，权重为1）
- $\sum$：对所有满足条件的边进行求和

**通俗理解：** 割就是"跨越两个社区的边的总权重"，这个值越小，说明两个社区之间的连接越少，划分越好。

**求解方法：** 存在多项式时间算法来求解最小割问题，特别是 **Edmonds-Karp 算法**，其时间复杂度为 $O(|V| \cdot |E|^2)$。

#### 4. 最小割 (Minimum-cut)

**目标：** 找到一个划分 $(A, B)$，使得 $cut(A,B)$ 的值最小。

$$\arg\min_{A,B} cut(A,B)$$

**公式解释：**
- $\arg\min$：表示"使得后面的值最小的参数"
- 即找到使 $cut(A,B)$ 最小的划分方式 $(A, B)$

**最小割的局限性：**

虽然最小割能找到连接最少的划分，但存在明显的问题：

- **只考虑簇间的联通性**：只关心两个社区之间有多少连接
- **不考虑簇内的连通性**：不关心每个社区内部的结构
- **可能产生不平衡的划分**：例如，将一个孤立的节点分离出来，只需要切断很少的边，但这样的划分是没有意义的

**举例说明：**
假设有一个图，其中有一个节点只通过一条边连接到主图，那么最小割会将这个节点单独分离出来（只需切断1条边），但这样的划分显然不合理。

#### 5. 归一化切割 (Normalized-cut)

为了解决最小割的问题，我们引入归一化切割，它同时考虑了簇间的连通性和各簇的规模。

##### 5.1 体积（Volume）的定义

首先需要定义社区 $A$ 的"体积" $vol(A)$：

$$vol(A) = \sum_{i \in A} k_i$$

**公式解释：**
- $k_i$：节点 $i$ 的度（degree），即连接到节点 $i$ 的所有边的权重之和
- $\sum_{i \in A}$：对社区 $A$ 中的所有节点求和
- $vol(A)$：社区 $A$ 中所有节点的度之和

**通俗理解：** $vol(A)$ 表示至少有一个端点在社区 $A$ 中的所有边的总权重。这个值反映了社区 $A$ 的"规模"或"密度"。

**举例说明：**
假设社区 $A$ 有3个节点：
- 节点1的度为 $k_1 = 5$（连接5条边）
- 节点2的度为 $k_2 = 3$（连接3条边）
- 节点3的度为 $k_3 = 4$（连接4条边）

那么 $vol(A) = 5 + 3 + 4 = 12$

##### 5.2 归一化切割的定义

$$ncut(A,B) = \frac{cut(A,B)}{vol(A)} + \frac{cut(A,B)}{vol(B)}$$

**公式解释：**
- 第一项 $\frac{cut(A,B)}{vol(A)}$：割的大小相对于社区 $A$ 的规模
- 第二项 $\frac{cut(A,B)}{vol(B)}$：割的大小相对于社区 $B$ 的规模
- 两项相加：综合考虑两个社区的规模

**为什么要归一化？**

通过除以各自的体积，我们将割的大小"标准化"了。这样：
- 如果一个社区很大（$vol$ 很大），即使割的值不变，归一化后的值也会变小
- 这就避免了将单个节点分离出来的情况（因为单个节点的 $vol$ 很小，归一化后的值会很大）

**优势：** 
- 使划分更加平衡，避免产生极小的社区
- 同时考虑了社区间的连接和社区的规模

**挑战：** 
- 计算归一化割是 NP-hard 问题
- 需要使用近似算法或启发式方法来高效地找到好的划分

#### 6. 练习题详解

[![image.png](https://i.postimg.cc/d0KTvKjj/image.png)](https://postimg.cc/0zcNdF3z)

**题目：** 对于给定的图(红色节点和绿色节点)，分别计算最优切割和最小切割的 $ncut$ 值。

##### 题目分析

从图中可以看到两种切割方式：
- **最优切割**（蓝色虚线）：在红色社区（左侧）和绿色社区（右侧）之间进行切割
- **最小切割**（红色虚线）：将右下角单个绿色节点孤立出来

##### 重要概念澄清

$vol(A)$ 的正确理解：**只计算社区内部的边的度数总和**，不包括被切割掉的跨社区的边。

- 对于节点 $i \in A$，其度 $k_i$ 只计算**连接到 $A$ 内其他节点的边**
- 被切割的边不计入 $vol$ 的计算

##### （1）最小切割的$ncut$值计算

**第一步：确定划分**
- 社区 $A$：右下角单个绿色节点
- 社区 $B$：其余所有节点

**第二步：计算 $cut(A,B)$**

观察图中，连接这个孤立节点和其他节点的边有 1 条（红色虚线），所以：
$$cut(A,B) = 1$$

**第三步：计算 $vol(A)$**

最小割后，这个节点完全孤立，社区 $A$ 内部没有任何边，所以：
$$vol(A) = 0$$

**注意：** 由于 $vol(A) = 0$，直接代入公式会导致分母为0，此时 $ncut$ 值趋向于无穷大：
$$ncut_{\text{min}} = \frac{cut(A,B)}{vol(A)} + \frac{cut(A,B)}{vol(B)} = \frac{1}{0} + \frac{1}{vol(B)} = +\infty$$

这说明**将单个节点完全孤立是一个极差的划分**！

##### （2）最优切割的ncut值计算

**第一步：确定划分**
- 社区 $A$：所有红色节点（左侧6个节点）
- 社区 $B$：所有绿色节点（右侧10个节点）

**第二步：计算 $cut(A,B)$**

观察图中，跨越蓝色虚线（连接红色和绿色节点）的边，有 **2条边** 跨越两个社区，所以：
$$cut(A,B) = 2$$

**第三步：计算 $vol(A)$**

计算红色社区内部所有节点的度之和（只计算社区内部的连接）：
- 红色区域内部边数 9 条
- 每条边贡献 2 个度（两个端点）
- $vol(A) = 2 \times 9 = 18$

**第四步：计算 $vol(B)$**

计算绿色社区内部所有节点的度之和（只计算社区内部的连接）：
- 绿色区域内部边数为 15 条
- $vol(B) = 2 \times 15 = 30$

**第五步：计算 $ncut$ 值**

$$ncut_{\text{optimal}} = \frac{cut(A,B)}{vol(A)} + \frac{cut(A,B)}{vol(B)}$$

$$ncut_{\text{optimal}} = \frac{2}{30} + \frac{2}{18} = \frac{1}{15} + \frac{1}{9} = \frac{3 + 5}{45} = \frac{8}{45} \approx 0.178$$

##### （3）结果对比与结论

| 切割方式 | $cut(A,B)$ | $vol(A)$ | $vol(B)$ | $ncut$ 值 |
|---------|-----------|----------|----------|-----------|
| 最小切割 | 1 | 0 | >0 | $+\infty$ |
| 最优切割 | 2 | 18 | 30 | 0.178 |

**结论：**

1. **最小切割的致命问题：**
   - 将单个节点孤立后，社区内部没有任何连接（$vol(A) = 0$）
   - $ncut$ 值趋向无穷大，这是最差的划分
   - 这正是为什么需要归一化切割的原因！

2. **最优切割的优势：**
   - 两个社区都有密集的内部连接
   - 虽然 $cut$ 值可能不是最小，但 $ncut$ 值很小
   - 实现了平衡且合理的划分

3. **归一化切割的意义：**
   - 通过考虑社区的规模，避免了不合理的极端划分
   - $ncut$ 值越小，说明划分越好
   - 这正是归一化切割优于简单最小切割的原因

### 二、边介数（Edge Betweenness）

#### 1. 边介数的定义

**边介数（Edge Betweenness）**：通过该边的最短路径的数量。

**作用：**
- 用于衡量图中一条边的重要性或中心性
- 反映图中有多少条最短路径经过该边

**重要性判断：**
- 若很多最短路径都经过该边，则该边对于保持图的高效连接性就非常重要
- 相反，如果仅少数最短路径经过该边，则该边的重要性就较低

**应用价值：**

边介数有助于识别图中的关键连接，即这些连接一旦断裂会显著影响图中节点之间的通信效率。

**示例：**

[![image.png](https://i.postimg.cc/L892K9J7/image.png)](https://postimg.cc/mt507GNN)

在下图中，不同的边具有不同的边介数值：
- 左侧的边：$b = 16$（有16条最短路径经过）
- 右侧的边：$b = 7.5$（有7.5条最短路径经过）

边介数越大，该边在网络中的重要性越高。

#### 2. Girvan-Newman方法（简称GN方法）

**GN方法定义：**

Girvan-Newman方法是一种基于边介数概念的层次聚类算法，适用于无向无权网络。

**算法流程：**

重复以下步骤直到没有边剩余：
1. **计算边介数**：计算网络中所有边的边介数
2. **移除边介数最高的边**：找到边介数最大的边并将其从图中删除
3. **重新计算**：在每个步骤后，需要重新计算剩余边的边介数

**输出结果：**
- 相连接的边构成社区
- 可输出网络的层次分解

**重要提示：** 在每个步骤，均需重新计算边介数，因为移除一条边会影响其他边的最短路径。

#### 3. GN方法案例分析

考虑下图所示的网络，应用GN方法进行社区划分：

**Step 1：** 计算所有边的边介数，移除边介数最高的边（边7-8，边介数为49）

结果：图被分成两个主要部分

**Step 2：** 重新计算剩余边的边介数，继续移除边介数最高的边

结果：进一步细分，形成更小的社区

**Step 3：** 持续迭代，直到所有边都被移除

结果：每个节点成为独立的社区

**最终输出：层次状的网络划分**

通过记录每次移除边的顺序，可以构建一个层次树（dendrogram），展示网络在不同粒度下的社区结构。

#### 4. 如何计算边介数

##### 4.1 基本方法

**方法：从起始节点开始，使用广度优先遍历（BFS）**

**步骤1：构建最短路径树**

从起始节点（如节点A）开始，使用BFS构建到所有其他节点的最短路径树，并标记每个节点的层次：
- 第0层：起始节点A
- 第1层：与A直接相连的节点（B, C, D, E）
- 第2层：距离A为2的节点（F, G, H）
- 第3层：距离A为3的节点（I, J）
- 第4层：距离A为4的节点（K）

**步骤2：计算最短路径数量**

计算从起始节点A到网络中其他每个节点的最短路径数量。

**公式：**

从A到某个节点X的最短路径数量 = 所有能到达X的父节点的最短路径数量之和

**示例：**
- 从A到I的最短路径数量 = 从A到F的最短路径数量 + 从A到G的最短路径数量
- 从A到K的最短路径数量 = 从A到I的最短路径数量 + 从A到J的最短路径数量

##### 4.2 自底向上计算边介数

**核心思想：** 如果存在多条最短路径，则可按比例划分边介数。

**算法步骤：**

1. **添加边流**
   - 初始化：每个节点的流 = 1 + 其所有子边的流之和
   - 根据父节点的值分配流

2. **对于每个起始节点U，重复广度优先搜索过程**

**详细计算规则：**

对于某条边 $(X, Y)$，其边介数的计算遵循以下规则：
- 如果从A到Y只有一条最短路径经过X，则该边获得完整的流
- 如果从A到Y有多条最短路径（通过不同的父节点），则按照各父节点的最短路径数量比例分配流

**具体案例：**

以节点H为例：
- A-H的最短路径总和算1
- H有2条可能的路径（通过G和通过D），需要平均分配
- 共有2条最短路径可被分配，因此每条边分配 $\frac{1}{2}$

以节点I为例：
- A-I的最短路径总和算1，外加经过I到K的0.5
- I有1.5可被分配，按照1:2的比例划分（因为到F有1条路径，到G有2条路径）

以节点K为例：
- A-K的最短路径总和算1
- K通过I和J两条路径到达，平均分配，每条边为 $\frac{1}{2}$

#### 5. 练习题

**题目：** 计算从节点B开始的路径的边介数

给定下图所示的网络结构，请：
1. 从节点B出发，使用广度优先遍历构建最短路径树
2. 计算从B到所有其他节点的最短路径数量
3. 自底向上计算每条边的边介数
4. 标注出边介数最高的边

**提示：**
- 首先确定各层节点（0层：B，1层：F,C,A，等等）
- 然后计算每个节点到B的最短路径数量
- 最后按比例分配流，计算边介数

---

**总结：**

- **边介数**衡量了边在网络中的重要性，通过该边的最短路径越多，边越重要
- **GN方法**通过迭代移除边介数最高的边来进行社区发现
- **计算方法**使用BFS构建最短路径树，然后自底向上按比例分配流
- 边介数的计算需要考虑多条最短路径的情况，按比例划分

### 三、模块度（Modularity）

**定义：**
模块度Q用于衡量网络社区划分的质量。

**公式：**
```
Q = (1/2m) Σ[Aᵢⱼ - (kᵢkⱼ/2m)]δ(cᵢ,cⱼ)
```

其中：
- m：边的总数
- Aᵢⱼ：邻接矩阵
- kᵢ：节点i的度
- δ(cᵢ,cⱼ)：节点i和j是否在同一社区

**特性：**
- Q值范围：[-0.5, 1]
- Q > 0.3通常表示有明显的社区结构
- 模块度最大化是NP困难问题

### 四、Louvain方法

Louvain算法是一种基于模块度优化的快速社区发现算法。

**算法步骤：**

**第一阶段（模块度优化）：**
1. 初始化：每个节点为一个独立社区
2. 遍历每个节点：
   - 尝试将该节点移动到邻居节点所在的社区
   - 计算模块度增益ΔQ
   - 选择使ΔQ最大且为正的移动
3. 重复步骤2直到模块度不再增加

**第二阶段（网络聚合）：**
1. 将同一社区的所有节点合并成一个超级节点
2. 社区间的边权重等于原节点间边权重之和
3. 对新网络重复第一阶段

**重复两个阶段直到模块度不再显著增加。**

**优势：**
- 时间复杂度：O(n log n)
- 能处理大规模网络（百万级节点）
- 能发现层次化社区结构
- 实现简单，效果好

**应用案例：**
- 社交网络社区检测
- 生物网络模块识别
- 知识图谱聚类

---

## 第九讲 子模函数及其应用

### 一、应用背景

子模函数在组合优化中有广泛应用，许多实际问题都具有子模性质。

**典型应用场景：**
- 信息传播最大化
- 传感器布置优化
- 文档摘要
- 特征选择
- 图像分割

### 二、子模函数（Submodular Function）

**定义：**
集合函数f: 2^V → ℝ 是子模的，如果对于任意A ⊆ B ⊆ V 和 x ∉ B，有：
```
f(A ∪ {x}) - f(A) ≥ f(B ∪ {x}) - f(B)
```

**直观理解：**
边际收益递减（Diminishing Returns）：向小集合添加元素的收益不小于向大集合添加相同元素的收益。

**常见子模函数：**
1. **覆盖函数**：f(S) = |∪ₛ∈S Cₛ|
2. **截断函数**：f(S) = min(|S|, k)
3. **图割函数**
4. **熵函数**

**性质：**
- 子模函数的非负线性组合仍是子模的
- 子模函数的最小化可以在多项式时间内精确求解
- 子模函数的最大化是NP困难的，但存在近似算法

### 三、集合覆盖问题（Set Cover Problem）

**问题定义：**
给定全集U和若干子集S₁, S₂, ..., Sₙ ⊆ U，找到最少数量的子集使其并集等于U。

**形式化：**
```
最小化：|S|
约束条件：∪ₛ∈S s = U
```

**复杂性：**
- NP困难问题
- 不存在常数因子近似算法（除非P=NP）

**贪心算法：**
1. 初始化：S = ∅, R = U（未覆盖元素）
2. 循环直到R = ∅：
   - 选择覆盖R中最多元素的集合s
   - S = S ∪ {s}
   - R = R \ s
3. 返回S

**性能保证：**
- 贪心算法的近似比：O(ln n)
- 这是可达到的最好近似比（在P≠NP假设下）

**子模性与贪心算法：**
集合覆盖函数f(S) = |∪ₛ∈S s|是子模的，对于单调子模函数最大化：
- 贪心算法保证(1 - 1/e)近似比
- 这个界是紧的

**实际应用：**
- **设施选址**：用最少的设施覆盖所有需求点
- **无线网络覆盖**：最小化基站数量
- **测试用例选择**：用最少的测试覆盖所有代码路径
- **特征选择**：选择最少的特征保持分类性能

**高级话题：**
- 加权集合覆盖
- 多目标集合覆盖
- 在线集合覆盖
- 分布式集合覆盖算法

---

## 课程总结

本课程系统地介绍了数学基础算法的核心内容，涵盖了从理论到实践的多个重要主题：

**基础理论部分（第1-2讲）：**
- **算法分析**为评估算法效率提供了数学工具，相似度搜索和高维空间技术是现代大数据应用的基础
- **尾不等式**提供了随机算法性能的概率保证，是设计和分析概率算法的重要理论基础

**数据流处理（第3-4讲）：**
- **数据流算法**解决了在有限空间内处理海量数据的问题，频繁元素检测和滑动窗口模型在实时系统中应用广泛
- **分布式数据流**扩展到多节点场景，聚集查询和top-k监控为大规模分布式系统提供了实用解决方案

**核心算法技术（第5-7讲）：**
- **哈希技术**为大规模数据的快速检索和去重提供了基础工具，布隆过滤器和LSH在实际应用中具有重要价值
- **线性规划与整数规划**是运筹优化的核心方法，单纯形算法、分支界定法和切平面法为求解复杂优化问题提供了理论基础
- **内存计算**代表了大数据处理的技术趋势，通过合理利用单机大内存和分布式架构可以显著提升计算性能

**图算法与优化（第8-9讲）：**
- **社区发现**算法帮助我们理解复杂网络的结构特征，Louvain方法在实践中表现出色
- **子模优化**为许多实际问题提供了理论保证的近似算法，贪心策略在子模函数优化中具有良好的性能保证

**核心思想总结：**
1. **近似与权衡**：在精确性和效率之间寻找平衡
2. **概率与随机化**：利用随机化技术突破确定性算法的局限
3. **分布式思维**：将算法扩展到大规模分布式环境
4. **理论保证**：为实际算法提供可证明的性能边界

这些算法和技术在实际应用中经常需要结合使用，深入理解其数学基础对于解决复杂的工程问题至关重要。掌握这些方法不仅能提升算法设计能力，更能在面对实际问题时选择合适的解决方案，在效率、精度和资源消耗之间做出明智的权衡。